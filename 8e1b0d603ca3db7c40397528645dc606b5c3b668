{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "e27cf697_864ab04a",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1024147
      },
      "writtenOn": "2023-07-25T08:36:29Z",
      "side": 1,
      "message": "+patrick, who might be interested in faster/leaner tag visibility.",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "208bf7c1_ec7e5cdc",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1026112
      },
      "writtenOn": "2023-07-25T11:14:39Z",
      "side": 1,
      "message": "The change LGTM overall. Roaring claims it\u0027s so much better than EWAH and others, so now I am curious and would care about some numbers. Can you benchmark this against one of your large repos and see what you get on either library?",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2a97643e_24f54f40",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1024147
      },
      "writtenOn": "2023-07-25T11:35:31Z",
      "side": 1,
      "message": "bonus points for data about memory usage.\n\n(The JGit reachability bitmap is causing some headaches for us, and we were wondering if we could reduce memory pressure by migrating to RB.)",
      "parentUuid": "208bf7c1_ec7e5cdc",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "09e79dbe_09517eae",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1002666
      },
      "writtenOn": "2023-07-25T21:22:06Z",
      "side": 1,
      "message": "Yeah, I\u0027m doing exactly that, but with this change backported to 3.5. I had tested with EWAH first when I originally made this improvement on our fork several years ago and RB was significantly better than EWAH, but I don\u0027t remember the details of how I tested it.\n\nSo far I can conclusively say that the on-disk storage size is *much* smaller with RB than with bitsets. A set of 30 kernel repos gives a git_tags cache size of ~550MB with bitsets and only ~115MB with RB.\n\nI\u0027d like to see that same kind of 5x improvement for memory, but I\u0027m not sure of the/a right way to precisely measure the memory difference. If I look at the \u0027memory\u0027 field logged in the sshd_log from a git ls-remote of a very large repo with 1.3M refs (including ~375K tags and ~26.5K other non-skippable refs), using bitsets logs ~230GB of memory and using RB logs ~214GB of memory.\n\nThe other impact I\u0027ve measured (but haven\u0027t repeated enough to feel sure about it) is that there does seem to be a small runtime performance impact to using RB (at least for these large repos).\n\nDo you have better ideas for how to get a representative memory impact with and without this change? My other idea so far is to try and drop the heapLimit to a point that it fails with one and works with the other ðŸ˜‚\n\nI think @prudhvi.alahari@linaro.org had gotten a jmh benchmark based on change 243053 working a while back, but it wasn\u0027t clear if that was an accurate way to measure memory impacts since running servers look a lot different than the benchmark.",
      "parentUuid": "2a97643e_24f54f40",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "fb01c3e6_12463b24",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1004034
      },
      "writtenOn": "2023-07-25T21:39:08Z",
      "side": 1,
      "message": "The memory field logged in the sshd_log is the memory allocated by this thread during the request\u0027s lifetime as reported by ThreadMxBean#getThreadAllocatedBytes [1].\n\nMaybe you can create a heap dump and use MAT [2] to analyse it.\n\n[1] https://gerrit-review.googlesource.com/c/gerrit/+/301213\n[2] https://eclipse.dev/mat",
      "parentUuid": "09e79dbe_09517eae",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "50e97535_99e87f00",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1002666
      },
      "writtenOn": "2023-07-25T22:10:33Z",
      "side": 1,
      "message": "\u003e The memory field logged in the sshd_log is the memory allocated by this thread during the request\u0027s lifetime as reported by ThreadMxBean#getThreadAllocatedBytes [1].\n\nI understand that, but I also thought that most JVM implementations don\u0027t really track memory at the per-thread level, so often what\u0027s reported here is all allocations during the life of this thread. Am I mistaken on that?\n\n\u003e \n\u003e Maybe you can create a heap dump and use MAT [2] to analyse it.\n\nI have been using MAT, but the heap sizes are large enough that\u0027s it\u0027s a bit challenging to use well as I\u0027m doing all the parsing on a machine without the Eclipse GUI running (using the ParseHeapDump script). There\u0027s also the timing aspect of getting a dump when the tags cache will be in it, but I can probably figure that out.\n\n\u003e My other idea so far is to try and drop the heapLimit to a point that it fails with one and works with the other ðŸ˜‚\n\nSo I did actually try this as it seemed pretty simple and it worked pretty well. With `-Xmx\u003d38g -Xms\u003d38g` both bitsets and RB are able to run `git ls-remote` on that huge repo with 1.3M refs. When I drop down to `-Xmx24g -Xms24g`, RB succeeds in 5.5 minutes, but bitsets OOM with a GC overhead limit exception after about 7 minutes. I used `-XX:+UseParallelOldGC -XX:ParallelGCThreads\u003d20` for both. Interestingly, this time that `memory` field only reported ~175GB with RB.",
      "parentUuid": "fb01c3e6_12463b24",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b9f1f181_15aceb1f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1002666
      },
      "writtenOn": "2023-07-25T23:47:47Z",
      "side": 1,
      "message": "And I have even better data now. I did get bitsets to work for that huge repo at -Xmx\u003d30g. It finished in 5 minutes 45 seconds and was **1.48gb** on disk (as reported by `gerrit show-caches`). However, I was able to maintain the 5 minutes 30 seconds with RB all the way down to -Xmx\u003d16g and it was only **58mb(!!!!)** on disk. Further, I tried -Xmx\u003d10g and it still worked with RB, but runtime increased to 6 minutes 30 seconds.\n\nI have heap dumps, so I\u0027ll run MAT and see what else I can tell.",
      "parentUuid": "50e97535_99e87f00",
      "revId": "8e1b0d603ca3db7c40397528645dc606b5c3b668",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    }
  ]
}