{
  "comments": [
    {
      "key": {
        "uuid": "b223204d_33c74374",
        "filename": "gerrit-acceptance-tests/src/test/java/com/google/gerrit/acceptance/server/change/PatchListCacheIT.java",
        "patchSetId": 1
      },
      "lineNbr": 217,
      "author": {
        "id": 1026112
      },
      "writtenOn": "2017-07-27T16:06:42Z",
      "side": 1,
      "message": "Dave, can I get your opinion here? I try to provoke JGit to throw a LargeObjectException, but it doesn\u0027t so I think I\u0027m holding this wrong.",
      "revId": "f1606a56847705e6a1c94a443d20b162e70c7bed",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "6dfb1020_bbea4abf",
        "filename": "gerrit-acceptance-tests/src/test/java/com/google/gerrit/acceptance/server/change/PatchListCacheIT.java",
        "patchSetId": 1
      },
      "lineNbr": 217,
      "author": {
        "id": 1010008
      },
      "writtenOn": "2017-07-28T11:39:17Z",
      "side": 1,
      "message": "(Disclaimer: I don\u0027t know how it\u0027s actually supposed to work.)\n\nSearching for bigFileTreshold and getBigFileThreshold in JGit/Gerrit, it looks like this is only used by the packer to determine whether to delta compress objects, it doesn\u0027t appear to be used elsewhere.\n\nI do see in ResolveMerger#getRawText where it\u0027s calling ObjectLoader#getCachedBytes(). Maybe that needs to be changed to use #getCachedBytes(int) and respect some configuration option? (Either core.bigFileThreshold or something else.)",
      "parentUuid": "b223204d_33c74374",
      "revId": "f1606a56847705e6a1c94a443d20b162e70c7bed",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "ea2730e4_4d55a866",
        "filename": "gerrit-acceptance-tests/src/test/java/com/google/gerrit/acceptance/server/change/PatchListCacheIT.java",
        "patchSetId": 1
      },
      "lineNbr": 217,
      "author": {
        "id": 1010008
      },
      "writtenOn": "2017-07-28T11:42:58Z",
      "side": 1,
      "message": "git-config(1) says:\n\n core.bigFileThreshold\n     Files larger than this size are stored deflated, without attempting delta\n     compression. Storing large files without delta compression avoids excessive\n     memory usage, at the slight expense of increased disk usage. Additionally files\n     larger than this size are always treated as binary.\n\nSo if the goal is for Gerrit to always treat large objects as binary, and to never attempt to merge their contents, then respecting this config option seems reasonable.\n\nOTOH, since JGit currently will happily content-merge large binary files as long as you have the RAM for it, this would technically be a regression. Perhaps this needs to be controlled in code with an option to ResolveMerger.",
      "parentUuid": "6dfb1020_bbea4abf",
      "revId": "f1606a56847705e6a1c94a443d20b162e70c7bed",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153",
      "unresolved": true
    }
  ]
}